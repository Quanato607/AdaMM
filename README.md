<h1 align="center"> N&nbsp;O&nbsp;&nbsp;M&nbsp;â¬¢&nbsp;D&nbsp;A&nbsp;L&nbsp;I&nbsp;T&nbsp;Y&nbsp;&nbsp;L&nbsp;E&nbsp;F&nbsp;T&nbsp;&nbsp;B&nbsp;E&nbsp;H&nbsp;I&nbsp;N&nbsp;D</h1>


<div align="center">

[![](https://img.shields.io/github/stars/Quanato607/MST-KDNet)](https://github.com/Quanato607/AdaMM)
[![](https://img.shields.io/github/forks/Quanato607/MST-KDNet)](https://github.com/Quanato607/AdaMM)
[![](https://img.shields.io/badge/project-page-red.svg)](https://github.com/Quanato607/AdaMM)
[![](https://img.shields.io/badge/arXiv-2403.01427-green.svg)](https://arxiv.org/abs/2030.12345)
</div>

This implementation of **No Modality Left Behind: Adapting to Missing Modalities via Knowledge Distillation for Brain Tumor Segmentation**. 

## ðŸŽ¥ Visualization for Implementation on Software

<div align="center">
<img src="https://github.com/Quanato607/MST-KDNet/blob/main/imgs/implementation.gif" width="90%">
</div>

## â¬¢ Related Works

<p align="center">
  <img src="./imgs/fig5.png" alt="Figure 3" width="90%">
</p>
<p align="center">
  <img src="./imgs/fig6.png" alt="Figure 3" width="90%">
</p>

> (a) **Data Generation** â€” An external generator synthesizes absent modalities, creating a complete four-channel input for the segmentation model.  (b) **Feature Generation** â€” The network learns to hallucinate modality-specific features internally when inputs are missing.  (c) **Sample Retrieval** â€” Retrieves training cases from modality-matched cohorts to substitute absent scans before segmentation.  (d) **Robustness Enhancement** â€” Trains with random modality dropout to segment directly from available scans.  (e) **Multi-task Learning** â€” An auxiliary decoder reconstructs absent modalities (red dashed arrows) while the main branch outputs the segmentation mask.  (f) **Knowledge Distillation** â€” A full-modality teacher guides a partial-modality student via feature and prediction alignment, improving accuracy under incomplete inputs.

## ðŸ§—Proposed method
<br><br>
![](./imgs/fig1.png)
<br><br>

> **Framework overview**  (A) **Missing-modality Sampling** â€” Generates 15 MRI modality combinations and leverages an Adapter Bank to compensate for absent inputs.  (B) **Knowledge-distillation Training** â€” Incorporates **BBDM**, **GARM**, and **LGRM**, with **GARM** applied exclusively to the student branch.

## â¬¢ Requirements

To install requirements:

```setup
pip install -r requirements.txt
```

## â¬¢ Training

To train our model in the paper, run this command:

```train
python train.py
```

<p align="center">
  <img src="./imgs/fig2.png" alt="Figure 3" width="100%">
</p>

> **Graph-guided Adaptive Refinement Module** - **Stage 0 â€“ Graphâ€“node Alignment** â€” Establishes consistent correspondence between nodes across modality-specific and shared graphs.   - **Stage 1 â€“ Cross-graph Enhancement** â€” Facilitates mutual information exchange to enrich node representations.   - **Stage 2 â€“ Graph Refinement & Feature Reprojection** â€” Progressively refines node embeddings and projects them back to the feature space for downstream processing.

## â¬¢ Evaluation

To evaluate our model in the paper, run this command:

```eval
python eval.py
```

<br><br>
![](./imgs/fig4.png)
<br><br>

> **Lesion-Presence-Guided Reliability Module** â€” For each slice, voxel-level probability maps from the student (left) and teacher (right) are aligned with the ground-truth label map (centre) using voxel-wise mean-squared error. At the same time, the three lesion classes (**NET**, **ET**, **ED**) are collapsed into existence scores, which are then matched to the binary presence vector of the label via an entropy loss.


## â¬¢ Diversity of Adapters

<p align="center">
  <img src="./imgs/fig10.png" alt="Figure 3" width="90%">
</p>

> **Cosine similarity heatmaps of adapter parameters** â€” Each four-digit code denotes available modalities in the order **[T1, T1Gd, T2, FLAIR]** (1 = present, 0 = absent). **(aâ€“c)** 1stâ€“3rd encoder adapters, **(dâ€“f)** 1stâ€“3rd decoder adapters. Deeper adapters exhibit higher inter-configuration similarity, suggesting convergence toward consistent full-modality feature approximation while preserving clear modality-combination specificity.

## â¬¢ Results of Performance

<p align="center">
  <img src="./imgs/fig9.png" alt="Figure 3" width="90%">
</p>

> **Scatter plots of model performance vs. parameter count** â€” Across four evaluation metrics: (a) **Dice (%)**, (b) **HD95 (mm)**, (c) **IoU (%)**, and (d) **Sensitivity (%)**. The proposed method (**Ours**) is highlighted in red.

## â¬¢ Results of Comparision Experiment

<p align="center">
  <img src="./imgs/fig8.png" alt="Figure 3" width="90%">
</p>

> **Qualitative comparison on the BraTS 2024 dataset** â€” Visualization of a randomly selected sample from the BraTS 2024 dataset, showing segmentation results under different modality-missing combinations across three axial views. The corresponding multimodal MRI sequences and ground truth are presented alongside model outputs. **Dice** and **HD95** metrics are provided for each modelâ€“combination pair. Our model (**Ours**) achieves superior segmentation accuracy and edge-control, with higher Dice scores and lower HD95 values across various missing-modality scenarios.  
**Color legend:** WT = red + yellow + green, TC = red + yellow, ET = red.

## â¬¢ Results of Ablation Experiment

<p align="center">
  <img src="./imgs/fig7.png" alt="Figure 3" width="90%">
</p>

> **Qualitative ablation study on the BraTS 2024 dataset** â€” Input images and ground-truth masks are shown for four modality configurations (**T1**, **T1+T1Gd+T2+FLAIR**, **T1Gd+T2**, and **T1+T1Gd+T2**). From left to right, the remaining columns display segmentation outputs and prediction heatmaps from models **without BBDM**, **without GARM**, **without LGRM**, and **with the complete model**. **Dice** scores for WT, TC, and ET are reported.

